\documentclass[a4paper,10pt, oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[style=ieee,backend=bibtex]{biblatex}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{lineno}
%\usepackage{showframe}
\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}
\usepackage{caption}
\usepackage{bytefield}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{lscape} 


\bibliography{informe_2}

	
\begin{document}
	
\begin{titlepage}
	\centering
	\includegraphics[width=0.25\textwidth]{../../Universidad_del_Litoral}\par\vspace{1cm}
	{\scshape\LARGE Universidad Nacional del Litoral \par}
	\vspace{1cm}
	{\scshape\Large Proyecto Final de Carrera\par}
	\vspace{1.5cm}
	{\huge\bfseries Diseño de un sistema de detección de anomalías en redes de computadoras.\par}
	\vspace{4cm}
	{\huge\bfseries Informe de avance 2\par}
	\vfill
	
	{\Large \itshape Pineda Leandro\par}
	
	
	% Bottom of the page
	\large Santa Fe\par
	{\large \today\par}	
\end{titlepage}

\modulolinenumbers[5]
\linenumbers

\section*{Resumen}
\newpage

\section{Introducción}
La detección en tiempo real de ciertos patrones en una red de datos tales cómo escaneo de puertos, ataques de denegación de servicio, expansión de \textit{malware}, entre otros, es de vital importancia para salvaguardar la integridad de la infraestructura de datos de cualquier organización. Estas anomalías pueden encontrarse analizando los flujos de datos y llevando cuenta de todos los paquetes que atraviesan los puntos de acceso a la red. En la capa de transporte, el protocolo TCP (que provee conexión host a host sobre IP) provee información suficiente para identificar estos patrones. Para llevar a cabo el análisis en tiempo real de los segmentos TCP que son transportados, utilizaremos el modelo de \textit{data streaming}: este se basa esencialmente en algoritmos que procesan una única vez los datos, dado que transferirlos y almacenarlos no es posible en la práctica, y permiten determinar cuales son los elementos más comunes en el stream de datos, así como algunos estadísticos (como la media, mediana, histograma), entre otros.

\section{El modelo de data streaming}

En este modelo, las entradas que van a ser procesadas no están disponibles para ser accedidas aleatoriamente desde disco o memoria, sino que llegan como uno o mas flujos continuos de datos. Los streams de datos son diferentes de los modelos relacionales convencionales en varios aspectos: 
\begin{itemize}
	\item Los elementos del stream deben ser procesados de manera \textit{online}.
	\item Los sistemas que procesan los datos no tienen control sobre el orden en los elementos de la entrada.
	\item Los stream de datos pueden ser infinitos.
	\item Una vez que un elemento es procesado, este se descarta.\footnote{En algunas aplicaciones los elementos pueden ser almanacenados para ser procesados posteriormente, pero en un modelo de streaming "puro" los elementos son procesados una única vez.}
\end{itemize}

Otra característica que es conveniente remarcar se relaciona con las consultas a los datos procesados: podemos hacer una distinción entre \textit{consultas únicas} y \textit{consultas continuas}\cite{Terry:1992:CQO:141484.130333}. Las consultas únicas (como aquellas que se hacen mediante un DBMS tradicional) son evaluadas una vez sobre un \textit{snapshot} de un conjunto de datos. La consultas continuas, por otro lado, son evaluadas continuamente mientras el stream de datos esta siendo procesado. Las respuestas a estas consultas pueden ser almacenadas y actualizadas en la medida que llegan nuevos flujos de datos, o pueden ser origen de otro stream de datos.

Podemos pensar a los segmentos TCP que pasan por un \textit{gateway} como los eventos o \textit{keys} a ser procesados. Aunque estos ocurren de manera secuencial, pertenecen a diferentes sesiones que están activas al mismo tiempo. Las \textit{keys} pueden ser agrupadas mediante la 5-tupla dirección de IP de origen, destino, número de puerto de origen y destino y protocolo. De esta manera se puede analizar el comportamiento del tráfico de red de cada una de las sesiones en busca de anomalías.
Sin embargo, el espacio de las \textit{keys} es tan grande que llevar registro de todos los eventos es imposible: consideremos el problema de determinar la frecuencia de ocurrencia de cierto evento, perteneciente a algún universo de eventos posibles $U$. Para obtenerla basta con llevar registro de la frecuencia $f_i$ por cada elemento $i \in U$: dado $U_0=\{a,b,c\}$ y una serie o stream de eventos $S=\{a,a,b,a,c,b,a\}$, la frecuencia de ocurrencia de cada elemento de $U_0$ es $f_a=4$, $f_b=2$ y $f_c=1$.  A pesar de su simpleza, el costo de memoria de este algoritmo crece exponencialmente cuando la cantidad de eventos posibles $|U|$ aumenta. En términos de implementación, para representar un universo de posibles elementos $U$ tal que $|U|=2^{40} \approx 10^{12}$ tenemos que se necesita almacenar en memoria $2^{40} * 32 \ bits\equiv 4096$ GB en contadores, uno por cada evento posible de $U$. Este tipo de problemas y similares llevaron al desarrollo de los llamados \textit{modelos de streaming} y la utilización de diferentes técnicas de conteo: bajo esta abstracción, los algoritmos procesan la entrada una única vez y deben calcular de manera precisa varios resultados usando recursos (espacio y tiempo por elemento) de forma estrictamente sublineal al tamaño de la entrada\cite{Muthukrishnan:2005:DSA:1166409.1166410}. Existen diferentes algoritmos para procesar y obtener información acerca de los eventos usando estructuras de datos que utilizan el espacio de memoria eficientemente. Sin embargo, estos métodos no calculan la frecuencia exacta de cada evento sino que la estiman: en general, para cantidades masivas de eventos basta con tener una buena aproximación de las frecuencias para identificar anomalías.

El problema de los eventos frecuentes consiste en procesar una serie consecutiva de elementos y encontrar aquellos que ocurren más frecuentemente en un período de tiempo. Es un problema muy estudiado en la minería en streams de datos debido a que la resolución de muchos problemas se basan directa o indirectamente en la identificación de eventos frecuentes.

Los algoritmos para encontrar elementos frecuentes pueden dividirse en dos clases. Aquellos basados en técnicas de conteo llevan registro de un subconjunto de elementos, y monitorean los contadores asociados con los mismos. Por cada entrada nueva, el algoritmo decide si guardar el elemento o no, y de hacerlo, con que valor lo inicializa. Por otro lado, los algoritmos basados en sketchs realizan proyecciones lineales aleatorias de las entradas\cite{Cormode:2008:FFI:1454159.1454225} (vistas como vectores de características) y por lo tanto, no almacenan explícitamente los elementos de entrada. Estos últimos tienen ciertas propiedades que son de gran utilidad para procesamiento de múltiples streams de datos.

\section{Detección de anomalías}
Cómo se mencionó anteriormente, existen muchos indicadores de escenarios que pueden tener impacto negativos en una infraestructura de red. Una práctica muy común en la etapa reconocimiento\footnote{El modelo de seguridad informática llamado \textit{cyber kill chain} describe las 7 etapas que todo atacante ejecuta para lograr su objetivo.\cite{hutchins2011intelligence}}, por donde comienzan todos los ataques, es el escaneo de puertos; uno o varios atacantes envían paquetes a un rango de puertos para determinar que servicios están activos, generando así grandes volúmenes de tráfico en la red. Otro escenario crítico es el de \textit{command and control (C2)} en donde el atacante tiene control de un conjunto de equipos ya infectados y utiliza sus recursos para atacar otro objetivo. Esto también genera tráfico anómalo y es un indicador crítico ante el cual se deben tomar medidas inmediatamente. Otro indicador son las fluctuaciones repentinas en los flujos de datos, que pueden indicar ataques de denegación de servicio (DoS).

La lista de ataques podría extenderse, pero de forma general pueden definirse dos tipos de comportamientos anómalos que son útiles para identificar amenazas. Para esto es necesario modelar el problema bajo el modelo de \textit{data streaming}.

\subsection{Modelado del problema}\label{modelado}

Consideremos los segmento TCP que atraviesa un punto de acceso; estos pueden ser representados como un \textit{stream} de eventos, donde cada elemento es una tupla $(x, v_x)$. El elemento $x$ pertenece a un dominio $T=\{0,1,2, \dots, n-1\}$ con $|T|=n$, y $v_x$ es un valor asociado a $x$. Para el caso de detección de eventos en tráfico de red, cada \textit{key} $x$ identifica un segmento TCP y está formado por la 5-tupla IP de origen, IP de destino, puerto de origen, puerto de destino y protocolo. El valor $v_x=1$ representa cantidad de paquetes identificados por $x$. Dado un \textit{stream} de eventos o \textit{keys}, definimos \textbf{\textit{heavy hitters}} como aquellos elementos que aparece más frecuentemente en el \textit{stream} de eventos. Los \textbf{\textit{heavy changers}} son aquellos elementos que presentan inconsistencias significativas entre el comportamiento observado y el comportamiento normal del flujo de datos (el cual se basa en lo ocurrido en el pasado) en un período de tiempo acotado\cite{Tong:2016:HTS:2927964.2927977}. 
En un algoritmo de detección de \textbf{\textit{heavy keys}}\footnote{Este término suele utilizase para referirse a ambos tipos de anomalías} típicamente se realizan dos procedimientos: en el primero, llamado de actualización, el valor de cada elemento es procesado y se almacenan los resultados en una estructura de datos; en el segundo, de detección, se examina la estructura de datos en cada época y se determinan los \textit{heavy keys}.

Para detectar \textit{heavy keys} se realizan estimaciones de frecuencias en ventanas de tiempo o épocas. En cada época, sea $S(x)$ la suma de los valores $v_x$ del elemento $x$. Sea $D(x)$ la diferencia (en valor absoluto) de $S(x)$ en la época actual y la anterior. Además, sea $U=\sum_{x \in T} S(x)$ la suma total de todos los elementos $x$ en una época. El problema de detectar \textit{heavy keys} consiste en encontrar aquellos elementos cuya suma o diferencias excedan, en valor absoluto, al parámetro $\phi$ en una época. Formalmente, definimos cómo \textit{heavy hitters} a aquellos elementos $x$ con $S(x) \geq \phi_1$, y \textit{heavy changers} a los elementos $x$ con $D(x) \geq \phi_2$. En adelante, referiremos indistintamente a los parámetros $\phi_1$ y $\phi_2$ como $\phi$, teniendo en cuenta que pueden ser diferentes.


\subsection{Elementos frecuentes en data streams} \label{elementos_frecuentes}
El problema de los elementos frecuentes es uno de los problemas más estudiados debido a su valor y a la simplicidad de su enunciado. Es importante en si mismo y como subrutina en procesos más complejos sobre \textit{streams} de datos. Antes de describir los algoritmos para encontrar elementos frecuentes es necesario enunciar formalmente el problema.

\subsubsection*{Elementos frecuentes} Dado un stream $S$ de $n$ elementos $t_1$, $t_2$, \dots, $t_n$, la frecuencia del elemento $i$ es $f_i = |\{j|t_j=i\}|$ (es decir, la cantidad de índices $j$ donde el $j$th elemento es $i$). Los $\phi$ elementos frecuentes están dados por $\{i|f_i>\phi n\}$.

\textbf{Ejemplo}: El stream $S=\{a,a,b,a,c,b,a\}$ tiene $f_a=4$, $f_b=2$ y $f_c=1$. Para $\phi=0.2$ los elementos frecuentes son $a$ y $b$.

\

Encontrar exactamente los $\phi$ elementos frecuentes puede ser costoso en términos de recursos: un algoritmo que resuelve el problema de los elementos frecuentes debe usar una cantidad lineal de espacio\cite{Cormode:2010:MFF:1731351.1731356}. Para relajar el requerimiento de recursos se utiliza una aproximación a la solución del problema.

\subsubsection*{$\epsilon$-aproximación de elementos frecuentes} 
Dado un stream $S$ de $n$ elementos una $\epsilon$-aproximación de los elementos frecuentes esta dada por el conjunto $F$ tal que todos los elementos $i \in F$ tengan frecuencia $f_i > (\phi - \epsilon)n$, y que no exista $i \notin F$ con $f_i > \phi n$. Dicho de otra manera, una $\epsilon$-aproximación de los elementos frecuentes consiste en encontrar todos los elementos con frecuencia mayor o igual a $\phi n$, de los cuales ninguno tiene frecuencia menor que $(\phi - \epsilon) n$.

\subsubsection*{Estimación de la frecuencia de un elemento}
Un problema relacionado a los anteriores consiste en estimar la frecuencia de los elementos al momento que se están procesando los datos. Dado un stream $S$ de $n$ elementos con frecuencias $f_i$, el problema de estimación de frecuencia consiste en procesar el stream de forma que para cualquier $i$ se pude obtener un $\hat{f_i}$ tal que $\hat{f_i} \leq f_i \leq \hat{f_i}+\epsilon n$.

\

Podemos encontrar los \textit{heavy hitters} resolviendo el problema de los elementos frecuentes. Los \textit{heavy changers} pueden ser identificados realizando estimaciones de frecuencia de los diferentes elementos para ventanas de tiempo adyacentes. Dada la naturaleza del problema es necesario implementar soluciones eficientes en el costo de cómputo como en costo de memoria.

\subsection{Métodos para determinar elementos frecuentes}
A continuación se describirán dos tipos de técnicas para encontrar elementos frecuentes: estas son las técnicas basadas en conteo y las basadas en sketchs.

\subsubsection{Técnicas basadas en conteo}

Los algoritmos basados en técnicas de conteo mantienen contadores para un subconjunto del universo de elementos posibles $T$ (ver \ref{modelado}). A continuación se describen algunos algoritmos de conteo existentes y sus características.

\subsubsection*{Majority}
El problema de los elementos frecuentes fue estudiado por primera vez en la década del 80, y fue enunciado de esta manera:
\begin{displayquote}
	Supongamos una lista de $n$ números, representando los "votos" de $n$ procesadores en el resultado de cierto cálculo. Queremos determinar si hay un voto mayoritario y cual es ese voto.\cite{GUIBAS1981208}
\end{displayquote}

Para solucionar el problema, los autores desarrollaron un algoritmo de una pasada llamado MAJORITY. El pseudocódigo de MAJORITY se describe a continuación: se almacena el primer elemento y se inicializa un contador en $1$. Por cada elemento subsecuente, si es el mismo que el elemento almacenado, se incrementa el contador en $1$. Si el elemento es diferente y el contador es $0$, entonces se reemplaza el elemento y se incrementa el contador en $1$. De lo contrario, se decrementa el contador. Luego de procesar todos los elementos, el algoritmo garantiza que si hay un voto mayoritario, entonces este debe ser el almacenado por el algoritmo. En el peor caso, el algoritmo realiza $2n$ comparaciones. Usando un argumento de paridad podemos concluir que el resultado del algoritmo es el correcto: si por cada elemento que no es el mayoritario tomamos uno de los mayoritarios, al final van a quedar solo elementos del conjunto mayoritario.

\subsubsection*{Frequent}

Una generalización del algoritmo MAJORITY es desarrolada en \cite{Karp:2003:SAF:762471.762473} la cual permite encontrar todos los elementos en el stream de datos que exceda $1/k$-ésimo del total de los elementos procesados. En lugar de guardar solo un elemento y un contador, FREQUENT almacena $k-1$ tuplas elemento-contador. Cada nuevo elemento es comparado contra todos los elementos almacenados por el algoritmo, y se incrementa el contador correspondiente si corresponde. Sino, si algún contador está en cero, se reemplaza el elemento y se inicializa el contador en $1$. Si los contadores de los $k-1$ elementos están siendo utilizados, entonces todos son decrementados en $1$. Este algoritmo no es exacto: al terminar de procesar el \textit{stream} de datos, el contador asociado con cada elemento esta a lo sumo $\epsilon n$ unidades por debajo del valor real si $k = 1/ \epsilon$.\cite{Kranakis03boundsfor}

FREQUENT puede resolver el problema de estimación de frecuencia desarrollado en la sección \ref{elementos_frecuentes} con $\epsilon = 1/k$. En el peor caso, el algoritmo es $O(k)$ en memoria y realiza $nk$ comparaciones y $nk$ asignaciones a variables.

\subsubsection{Técnicas basadas en sketches}

Los \textit{sketch} son estructuras de datos que pueden ser pensadas como proyecciones lineales de los datos de entrada. Si pensamos los elementos del \textit{stream} de datos como vectores cuya $i$-ésima entrada es $f_i$, el \textit{sketch} es el producto de este vector y una matriz. Los algoritmos usan una función de \textit{hash} con ciertas propiedades para definir la proyección lineal.

Los algoritmos de basados en \textit{sketches} resuelven el problema de estimación de frecuencia, pero necesitan información adicional para resolver el problema de los elementos frecuentes. Por esto, se suele aumentar la estructura de datos de los \textit{sketch} con algún método de conteo para encontrar elementos frecuentes de manera eficiente.

\subsubsection*{CountMin Sketch}

El algoritmo COUNTMINT\cite{Cormode:2005:IDS:1073713.1073718} consiste en un arreglo de contadores de $d \times w$, y $d$ funciones de hash independientes de a pares $h_j$ (una por cada fila del arreglo) con $1 \leq j \leq d$, que mapean elementos a $\{0, 1 \dots, w-1 \}$. Por cada nuevo elemento que es procesado, se actualizan $d$ contadores en el arreglo, incrementando su contador en $1$. Para estimar la frecuencia del elemento $i$, tenemos que $\hat{f_i} = min_{1 \leq j \leq d} C[j,h_j(i)]$; el error de esta estimación está acotado por desigualdad de Markov: cada contador $j$ sobre-estima $f_i$ a lo sumo en $n/w$, y reduce la probabilidad de error exponencialmente en $d$. Para $d=\log {1 \over \delta}$ y $w = O({1 \over \epsilon})$, el error en la estimación de $f_i$ es de a lo sumo $\epsilon n$ con probabilidad no menor a $1-\delta$. Finalmente, COUNTMIN es $O({1 \over \epsilon} \log {1 \over \delta})$ en términos de memoria y para realizar operaciones de actualización es $O(\log {1 \over \delta})$

\



 Dado el parámetro $l$ que define el tamaño del subconjunto de elementos, se define un diccionario $A$ de a lo sumo $l-1$ elementos. Supongamos un stream de datos $\{(x,1)\}$ (que puede ser infinito). Si el elemento $x$ está presente en $A$ entonces su valor es actualizado con $v_x$: $A[x]+=1$. Por el contrario, si la cantidad de elementos de $A$ es menor a $l-1$ se agrega el contador $A[x]=v_x$. En caso que la cantidad de elementos de $A$ sea igual a $l-1$, todos los valores de $A$ son decrementados en $1$: $A[x]-=1 \ \forall x \in A$
\newpage
\nocite{*}
\printbibliography
\end{document}

